# AI Code Generator

This project is an AI-assisted code generation application that uses Llama to generate Python code based on user prompts.
It is done as a test of my internship application into S4E.

## Overview

This application provides a web interface where users enter a prompt. 
Then, get Python code regarding the prompt that extends the Job class and a title for the code generated by the Llama.

### Önemli Not

Uygulamanın geliştirilmesi sırasında Ollama servisi pod'ları ayakta ve çalışıyor olmasına rağmen, sistemin kalanı ile iletişim kuramaması sorunu yaşadım.

Bu yüzden, sitede "Generate Code" butonuna bastıldığında 
"# An error occurred: 404 Client Error: Not Found for url: http://ollama-service:11434/api/generate"
şeklinde hata aldım. 

Bunu atlatabilmek için doğru Ollama modelini kullandığımdan emin olmaya çalıştım. 
Kullandığım Llama 2 modelinin doğru şekilde indirili olduğundan ve modele de Ollama sistemine de doğru şekilde erişilmeye çalışıldığından emin olmaya çalıştım. 

"kubectl get pods" ve "kubectl get pvc" komutlarını kullanarak Ollama'nın da bulunduğu pod'u kontrol ettiğimde sorunsuz çalıştığını gördüm.

Bu sorunu, Ollama modelini Llama 3.2 ile Llama 2 arasında değiştirmek de dahil olmak üzere farklı yöntemlerle çözmeye çalıştım fakat işe yaramadı. 
Bu yüzden erişeceğiniz web sitesi, bahsettiğim şekilde hata verme ihtimaline sahiptir. 

### Features

Project is done using Flask, Llama 2 of Ollama, Minikube, Kubectl and DockerHub.

DockerHub, as wanted in the mail, can be found at elifdenizgoztok/code-generator repository in DockerHub.
https://hub.docker.com/r/elifdenizgoztok/code-generator

If you cannot find the DockerHub repository, please reach out to me. 

## Architecture

Web UI (Browser) - Flask API Server - Ollama (Llama 2)

The application has:
- A web interface
- A Flask web server serving the frontend and API
- Ollama running locally or in a container to host Llama 2

## Prerequisites

To run the app locally or deploy it, you'll need:
- Docker and Docker Compose
- Minikube
- kubectl
- Ollama installed (if running locally without Docker)

## Getting Started

### Local Development

1. Clone the repository:
   git clone <repository-url>
   cd ai-code-generator

2. Create a `.env` file based on the example:
   cp env.example .env

3. Choose how you want to run it:

   **Option 1: Using Docker Compose**
   docker-compose up --build

   **Option 2: Running locally**
    Make sure Ollama is installed and running
   ollama pull llama2

    Install Python dependencies
   pip install -r requirements.txt

    Start the Flask app
   python app.py

4. Open your browser and go to: [http://localhost:8080](http://localhost:8080)

### Kubernetes Deployment with Minikube

#### Automated Deployment

To make setup easier, I added scripts (`deploy.bat` for Windows and `deploy.sh` for Linux/macOS).  
They automatically check for Minikube and kubectl, build and load the Docker image into Minikube, deploy Ollama, and display the service URL.

#### Manual Deployment

1. Start Minikube:
   minikube start

2. Build and load the Docker image:
   docker build -t code-generator:latest .
   minikube image load code-generator:latest

3. Deploy the services:
   kubectl apply -f k8s/ollama.yaml
   kubectl apply -f k8s/code-generator.yaml

4. Get the service URL:
   minikube service code-generator-service --url

## API Endpoints

- `GET /` – Loads the web interface
- `POST /generate` – Generates Python code based on a user prompt
  - Request body: `{ "prompt": "Your prompt here" }`
  - Response: `{ "title": "Generated title", "code": "Generated code" }`

## Implementation Details

### Project Structure

── app.py                     # Main Flask app
── llm_client.py               # Ollama client for Llama 2
── templates/                  # HTML templates
── index.html              # Web UI
── s4e/                        # Project package
   ── __init__.py
   ── config.py
   ── task.py
   ── job.py                  # Base Job class
── k8s/                        # Kubernetes configs
   ── code-generator.yaml
   ── ollama.yaml
── deploy.sh                   # Deployment script for Linux/macOS
── deploy.bat                  # Deployment script for Windows
── Dockerfile                  # Docker build file
── requirements.txt            # Python dependencies
── README.md                   


## How It Works

1. The user enters a prompt on the web page.
2. The Flask server sends the prompt to the LLM client.
3. The client adds a system prompt with a `Job` class template and forwards it to Llama 2.
4. Llama generates a Python class extending the `Job` class.
5. The server extracts the generated code and title from the response.
6. The result is shown on the web interface.

## License

MIT License
